{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, BatchNormalization, LeakyReLU, Dropout, \n",
    "    Bidirectional, LSTM, Dense, Flatten, Add, Layer, MultiHeadAttention, \n",
    "    Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Import joblib for saving the scaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Enable dynamic memory growth for the GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU '{gpus[0].name}' is being used for training.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training will be performed on the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Multi-Head Self-Attention Layer\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads=4):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.layernorm = BatchNormalization()\n",
    "        self.dropout = Dropout(0.1)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout(attn_output, training=training)\n",
    "        out = self.layernorm(inputs + attn_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file):\n",
    "    \"\"\"\n",
    "    Load OHLCV data from a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fibonacci_retracement(df, period=100):\n",
    "    \"\"\"\n",
    "    Computes Fibonacci Retracement levels based on the highest high and lowest low over a specified period.\n",
    "    \"\"\"\n",
    "    df['Fib_Max'] = df['high'].rolling(window=period).max()\n",
    "    df['Fib_Min'] = df['low'].rolling(window=period).min()\n",
    "    df['Fib_Diff'] = df['Fib_Max'] - df['Fib_Min']\n",
    "    df['Fib_23.6'] = df['Fib_Max'] - df['Fib_Diff'] * 0.236\n",
    "    df['Fib_38.2'] = df['Fib_Max'] - df['Fib_Diff'] * 0.382\n",
    "    df['Fib_50.0'] = df['Fib_Max'] - df['Fib_Diff'] * 0.5\n",
    "    df['Fib_61.8'] = df['Fib_Max'] - df['Fib_Diff'] * 0.618\n",
    "    return df\n",
    "\n",
    "def compute_indicators(df):\n",
    "    \"\"\"\n",
    "    Compute the specified technical indicators and add them to the DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame is sorted by timestamp\n",
    "    df = df.sort_index().reset_index()\n",
    "    \n",
    "    # Compute RSI\n",
    "    df['RSI'] = ta.rsi(df['close'], length=14)\n",
    "    \n",
    "    # Compute MACD\n",
    "    macd = ta.macd(df['close'])\n",
    "    df['MACD'] = macd['MACD_12_26_9']\n",
    "    df['MACD_signal'] = macd['MACDs_12_26_9']\n",
    "    df['MACD_hist'] = macd['MACDh_12_26_9']\n",
    "    \n",
    "    # Compute Bollinger Bands\n",
    "    bb = ta.bbands(df['close'], length=20, std=2)\n",
    "    df['Bollinger_High'] = bb['BBU_20_2.0']\n",
    "    df['Bollinger_Low'] = bb['BBL_20_2.0']\n",
    "    \n",
    "    # Volume\n",
    "    df['Volume'] = df['volume']\n",
    "    \n",
    "    # On-Balance Volume (OBV)\n",
    "    df['OBV'] = ta.obv(df['close'], df['volume'])\n",
    "    \n",
    "    # Moving Averages (50-period and 200-period)\n",
    "    df['MA50'] = ta.sma(df['close'], length=50)\n",
    "    df['MA200'] = ta.sma(df['close'], length=200)\n",
    "    \n",
    "    # Fibonacci Retracement Levels\n",
    "    df = compute_fibonacci_retracement(df, period=100)\n",
    "    \n",
    "    # Ichimoku Cloud\n",
    "    ichimoku = ta.ichimoku(df['high'], df['low'], df['close'])\n",
    "    ichimoku_filtered = ichimoku[0][['ISA_9', 'ISB_26']]\n",
    "    ichimoku_filtered_shifted = ichimoku_filtered.shift(-1)\n",
    "    df['Ichimoku_Cloud_a'] = ichimoku_df['ISA_9']\n",
    "    df['Ichimoku_Cloud_b'] = ichimoku_df['ISB_26']\n",
    "    \n",
    "    # Stochastic Oscillator\n",
    "    stoch = ta.stoch(df['high'], df['low'], df['close'])\n",
    "    df['Stochastic_Oscillator'] = stoch['STOCHk_14_3_3']\n",
    "    \n",
    "    # Average Directional Index (ADX)\n",
    "    adx = ta.adx(df['high'], df['low'], df['close'])\n",
    "    df['ADX'] = adx['ADX_14']\n",
    "    \n",
    "    # Drop intermediate Fibonacci calculation columns\n",
    "    df.drop(['Fib_Max', 'Fib_Min', 'Fib_Diff'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop rows with NaN values resulting from indicator calculations\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, target_column='close'):\n",
    "    \"\"\"\n",
    "    Prepare features and target variable for AI modeling.\n",
    "    \"\"\"\n",
    "    # Define feature columns (all indicators)\n",
    "    feature_columns = ['RSI', 'MACD', 'MACD_signal', 'MACD_hist',\n",
    "                       'Bollinger_High', 'Bollinger_Low',\n",
    "                       'Volume', 'OBV', 'MA50', 'MA200',\n",
    "                       'Fib_23.6', 'Fib_38.2', 'Fib_50.0', 'Fib_61.8',\n",
    "                       'Ichimoku_Cloud_a', 'Ichimoku_Cloud_b',\n",
    "                       'Stochastic_Oscillator', 'ADX']\n",
    "    \n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column].shift(-1)  # Predict the next close price\n",
    "    \n",
    "    # Drop the last row as it has NaN target\n",
    "    X = X[:-1]\n",
    "    y = y[:-1]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def normalize_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Normalize the feature data using StandardScaler.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    \"\"\"\n",
    "    Create sequences of data for LSTM input.\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_seq.append(X[i:i+sequence_length])\n",
    "        y_seq.append(y[i+sequence_length])\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_cnn_lstm_attention_model(input_shape, embed_dim=64, num_heads=4, dropout_rate=0.2, l2_reg=1e-4):\n",
    "    \"\"\"\n",
    "    Build and compile an Enhanced Hybrid CNN-LSTM model with Multi-Head Self-Attention.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers with Batch Normalization and LeakyReLU activation\n",
    "    x = Conv1D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv1D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    x = Conv1D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv1D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Residual Connection\n",
    "    res = Conv1D(filters=256, kernel_size=1, padding='same')(inputs)\n",
    "    res = MaxPooling1D(pool_size=2)(res)\n",
    "    x = Add()([x, res])\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Bidirectional LSTM layers with Batch Normalization and LeakyReLU activation\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Multi-Head Self-Attention\n",
    "    x = MultiHeadSelfAttention(embed_dim=256, num_heads=num_heads)(x)\n",
    "\n",
    "    # Additional Bidirectional LSTM Layer\n",
    "    x = Bidirectional(LSTM(100, return_sequences=False, kernel_regularizer=l2(l2_reg)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Fully connected layers with Dropout and L2 Regularization\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Compile the model with Adam optimizer\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot the actual vs predicted prices.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.plot(y_true, label='Actual Price')\n",
    "    plt.plot(y_pred, label='Predicted Price')\n",
    "    plt.title('Actual vs Predicted Close Prices')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the data\n",
    "    csv_file = 'BTC_USDT_1h_data.csv'  # Replace with your CSV file path\n",
    "    df = load_data(csv_file)\n",
    "    print(f\"Loaded data with {len(df)} records.\")\n",
    "    \n",
    "    # Compute indicators\n",
    "    try:\n",
    "        df = compute_indicators(df)\n",
    "        print(f\"Computed indicators. Dataset now has {len(df)} records.\")\n",
    "    except (TypeError, KeyError) as e:\n",
    "        print(f\"Error computing indicators: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_dataset(df, target_column='close')\n",
    "    print(f\"Prepared dataset with {X.shape[0]} samples.\")\n",
    "    \n",
    "    # Split into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False)\n",
    "    print(f\"Split data into {X_train.shape[0]} training and {X_test.shape[0]} testing samples.\")\n",
    "    \n",
    "    # Normalize features\n",
    "    X_train_scaled, X_test_scaled, scaler = normalize_features(X_train, X_test)\n",
    "    print(\"Normalized feature data.\")\n",
    "    \n",
    "    # Create sequences for LSTM\n",
    "    sequence_length = 60  # Number of past hours to consider\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, sequence_length)\n",
    "    print(f\"Created sequences with sequence length {sequence_length}.\")\n",
    "    \n",
    "    # Check if sequences have been created correctly\n",
    "    if X_train_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:\n",
    "        print(\"Error: Sequence creation resulted in empty arrays. Adjust the sequence length or check the data.\")\n",
    "        return\n",
    "    \n",
    "    # Build the Enhanced CNN-LSTM-Attention model\n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    model = build_enhanced_cnn_lstm_attention_model(input_shape)\n",
    "    model.summary()\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "    checkpoint = ModelCheckpoint('best_enhanced_trading_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    \n",
    "    # Train the model\n",
    "    epochs = 200\n",
    "    batch_size = 32\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop, checkpoint, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Load the best saved model\n",
    "    model = load_model('best_enhanced_trading_model.h5', custom_objects={'MultiHeadSelfAttention': MultiHeadSelfAttention})\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # Invert normalization of predictions and actual values\n",
    "    # Assuming 'close' was not scaled separately, otherwise adjust accordingly\n",
    "    # For simplicity, assuming 'close' was not scaled\n",
    "    # If 'close' was scaled, use the appropriate scaler to inverse transform\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plot_predictions(y_test_seq, y_pred)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    mse_value = mse(y_test_seq, y_pred).numpy()\n",
    "    mae_value = mae(y_test_seq, y_pred).numpy()\n",
    "    print(f\"Model Evaluation:\\nMSE: {mse_value:.4f}\\nMAE: {mae_value:.4f}\")\n",
    "    \n",
    "    # Save the enhanced model and scaler for future use\n",
    "    model.save('enhanced_cnn_lstm_attention_trading_model.h5')\n",
    "    print(\"Saved Enhanced CNN-LSTM-Attention model to 'enhanced_cnn_lstm_attention_trading_model.h5'.\")\n",
    "    \n",
    "    joblib.dump(scaler, 'scaler.joblib')\n",
    "    print(\"Saved scaler to 'scaler.joblib'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
